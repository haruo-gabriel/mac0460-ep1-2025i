{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTRUÇÕES\n",
    "\n",
    "1. Preencha as informações de identificação na célula abaixo antes de fazer a entrega\n",
    "2. Entregue apenas este **notebook com as partes requeridas preenchidas e com os resultados da execução** de todas as células com código\n",
    "3. Não altere o nome e parâmetros das funções.\n",
    "4. Dúvidas devem ser postadas no fórum do e-disciplinas (enquanto não encontramos outra alternativa)\n",
    "5. A data de entrega é pra valer\n",
    "6. Se estas intruções precisarem ser alteradas, será enviado um aviso via Quadro de Avisos do e-disciplinas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDENTIFICAÇÃO\n",
    "\n",
    "* Escreve o seu nome\n",
    "  - **Nome:**\n",
    "  \n",
    "  \n",
    "* Aluno regular de pós, aluno especial na pós, aluno regular de graduação, aluno especial na graduação, ouvinte, outro (especificar) -- indique a sua categoria\n",
    "  - **Categoria:** \n",
    "  \n",
    "  \n",
    "* \"Ao submeter este EP, declaro que o que está sendo entregue é fruto de meu próprio trabalho, e que não recorri a formas anti-éticas para fazer o EP.\" Responda SIM ou NÃO\n",
    "  - **Declaração:**\n",
    "  \n",
    "  \n",
    "* Caso tenha se baseado em **material externo** para fazer o EP e o material consultado tenha sido preponderante para fazer o EP, inclua as referências consultadas. Descreva (brevemente) abaixo para quê ou de que forma cada um deles foi útil. Se você usou ChatGPT ou similares, escreva sobre isso também.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAC0460 / MAC5832 (2025)\n",
    "<hr>\n",
    "\n",
    "# EP1: Linear regression - analytic solution\n",
    "\n",
    "### Objectives:\n",
    "\n",
    "- to implement and test the analytic solution for the linear regression task (see, for instance, <a href=\"http://work.caltech.edu/slides/slides03.pdf\">Slides of Lecture 03</a> of *Learning from Data*)\n",
    "- to understand the core idea (*optimization of a loss/cost function*) for parameter adjustment in machine learning\n",
    "\n",
    "### What to do:\n",
    "- some cells of this notebook must be filled. Places to be filled are indicated as:\n",
    "<code>\n",
    "    # START OF YOUR CODE\n",
    "    \n",
    "    \\# END OF YOUR CODE\n",
    "</code> \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset \n",
    "\n",
    "Here we will use the *Boston Housing Dataset*. See a description of the dataset at https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html. The dataset was downloaded from Kaggle and it is made available with this notebook (file HousingData.csv)\n",
    "\n",
    "Below is the list of the dataset variables (features). We will use MEDV as the prediction target (output). The remaining variables can be used as input features.\n",
    "<pre>\n",
    "        CRIM - per capita crime rate by town\n",
    "        ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "        INDUS - proportion of non-retail business acres per town.\n",
    "        CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "        NOX - nitric oxides concentration (parts per 10 million)\n",
    "        RM - average number of rooms per dwelling\n",
    "        AGE - proportion of owner-occupied units built prior to 1940\n",
    "        DIS - weighted distances to five Boston employment centres\n",
    "        RAD - index of accessibility to radial highways\n",
    "        TAX - full-value property-tax rate per $10,000\n",
    "        PTRATIO - pupil-teacher ratio by town\n",
    "        B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "        LSTAT - % lower status of the population\n",
    "        MEDV - Median value of owner-occupied homes in $1000\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset. See https://pandas.pydata.org/ to learn about pandas\n",
    "df=pd.read_csv('HousingData.csv')\n",
    "\n",
    "# show first instances of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show basic info regarding variables and dataset size\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some basic statistics of the dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting NaN values per column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The missing data (NaN) will be filled with the average value of the respective column. Since our aim in this EP\n",
    "# is to learn the linear regression algorithm, we will not discuss whether this decision is adequate or not.\n",
    " \n",
    "df = df.fillna(df.mean())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for the linear regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features: CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT\n",
    "\n",
    "# choose one input feature\n",
    "colname = 'RM'\n",
    "X = df.loc[:, colname]\n",
    "X = X.to_numpy().reshape(-1,1)\n",
    "X = np.concatenate([np.ones((len(X), 1)), X], axis = 1)  # data matrix, with left column of ONEs\n",
    "\n",
    "# target feature\n",
    "y = df.loc[:, 'MEDV']\n",
    "y = y.to_numpy()\n",
    "y = y.reshape((-1,1))\n",
    "\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)\n",
    "\n",
    "# Plot the chosen (X,y) dataset\n",
    "plt.scatter(X[:,1], y, alpha=0.4)\n",
    "plt.xlabel(colname)\n",
    "plt.ylabel('median value in $1000')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression recap\n",
    "\n",
    "Given a dataset $\\{(\\mathbf{x}^{(1)}, y^{(1)}), \\dots ,(\\mathbf{x}^{(N)}, y^{(N)})\\}$ with $\\mathbf{x}^{(i)} \\in \\mathbb{R}^{d}$ and $y^{(i)} \\in \\mathbb{R}$, we consider the family of linear models consisting of functions of the form\n",
    "$$\n",
    "h(\\mathbf{x}^{(i)}; \\mathbf{w}) = w_0 + \\sum_{j=1}^{d} w_jx_j\n",
    "$$\n",
    "where $\\mathbf{w} = (w_0, w_1, \\ldots, w_d)$.\n",
    "\n",
    "Note that $h(\\mathbf{x}^{(i)}; \\mathbf{w})$ is, in fact, an  [affine transformation](https://en.wikipedia.org/wiki/Affine_transformation) of  $\\mathbf{x}^{(i)}$. As commonly done, we will use the term \"linear\" to refer to an affine transformation.\n",
    "\n",
    "The output of $h$ is a linear transformation of $\\mathbf{x}^{(i)}$. We use the notation $h(\\mathbf{x}^{(i)}; \\mathbf{w})$ to make clear that $h$ is a parametric model, i.e., the transformation $h$ is defined  by the parameters $\\mathbf{w}$. We can view vector $\\mathbf{w}$ as a *weight* vector that controls the effect of each *feature* in the prediction.\n",
    "\n",
    "By adding one component with value equal to 1 to the instances $\\mathbf{x}$ (an artificial coordinate), we have:\n",
    "\n",
    "$$\\tilde{\\mathbf{x}} = (1, x_1, \\ldots, x_d) \\in \\mathbb{R}^{1+d}$$\n",
    "\n",
    "and then we can simplify the notation:\n",
    "$$\n",
    "\\hat{y}^{(i)} = h(\\mathbf{x}^{(i)}; \\mathbf{w}) = \\mathbf{w}^\\top  \\tilde{\\mathbf{x}}^{(i)}\n",
    "$$\n",
    "\n",
    "We would like to determine the optimal parameters $\\mathbf{w}$ such that prediction $\\hat{y}^{(i)}$ is as closest as possible to $y^{(i)}$ according to some error metric. Adopting the *mean square error* (MSE) as such metric we have the following cost function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^{N}\\big(\\hat{y}^{(i)} - y^{(i)}\\big)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "Thus, our task is to determine a function $h$ that minimizes $\\mathcal{L}(\\mathbf{w})$.\n",
    "\n",
    "To know more about MSE ( also referred to as *mean <span style=\"color: red\">squared</span> error* ) see for instance its [explanation on Wikipedia](https://en.wikipedia.org/wiki/Mean_squared_error).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The solution\n",
    "\n",
    "( See Section 3.2 of the textbook (or [Lecture 3](http://www.youtube.com/watch?v=FIbVs5GbBlQ)), or Section 2 of  [this text](https://sgfin.github.io/files/notes/CS229_Lecture_Notes.pdf), for more details )\n",
    "\n",
    "Given $f:\\mathbb{R}^{N\\times d} \\rightarrow \\mathbb{R}$ and $\\mathbf{A} \\in \\mathbb{R}^{N\\times d}$, we define the gradient of $f$ with respect to $\\mathbf{A}$ as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\nabla_{\\mathbf{A}}f = \\frac{\\partial f}{\\partial \\mathbf{A}} =  \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial \\mathbf{A}_{1,1}} & \\dots & \\frac{\\partial f}{\\partial \\mathbf{A}_{1,d}} \\\\\n",
    "\\vdots &  \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial \\mathbf{A}_{n,1}} &  \\dots & \\frac{\\partial f}{\\partial \\mathbf{A}_{n,d}}\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Let $\\mathbf{X} \\in \\mathbb{R}^{N\\times (1+d)}$ be a matrix (sometimes also called the *design matrix*) whose rows are the extended instances of the input data and let $\\mathbf{y} \\in \\mathbb{R}^{N}$ be the vector consisting of all values $y^{(i)}$ (i.e., $\\mathbf{X}{[i,:]} = \\mathbf{\\tilde{x}}^{(i)}$ and $\\mathbf{y}{[i]} = y^{(i)}$). It can be verified that: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\mathbf{w}) = \\frac{1}{N}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})^{T}(\\mathbf{X}\\mathbf{w} - \\mathbf{y})\n",
    "\\end{equation}\n",
    "\n",
    "To find the parameter $\\mathbf{w}$ that minimizes the loss $\\mathcal{L}(\\mathbf{w})$ , using basic matrix derivative concepts we can compute the gradient of $\\mathcal{L}(\\mathbf{w})$ with respect to $\\mathbf{w}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\mathbf{w}}\\mathcal{L}(\\mathbf{w}) = \\frac{2}{N} (\\mathbf{X}^{T}\\mathbf{X}\\mathbf{w} -\\mathbf{X}^{T}\\mathbf{y})   \n",
    "\\end{equation}\n",
    "\n",
    "Thus, when $\\nabla_{\\mathbf{w}}\\mathcal{L}(\\mathbf{w}) = 0$ we have \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}^{T}\\mathbf{X}\\mathbf{w} = \\mathbf{X}^{T}\\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "Hence,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w} = (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "This is a straighforward way to solve the esquation and find the parameter values of $\\mathbf{w}$. Note that this solution has a high computational cost. As the number of variables (*features*) increases, the cost for matrix inversion becomes prohibitive.\n",
    "There are other ways to solve linear equation systems, but they are out of scope of this course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>1. Write the weight computation function</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights(X, y):\n",
    "    \"\"\"\n",
    "    Computes the weights of a linear function using the normal equation method.\n",
    "    X is the input data matrix, already with the leftmost column with 1s.\n",
    "\n",
    "    :param X: input data matrix\n",
    "    :type X: np.ndarray(shape=(N, 1+d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :return: weight vector\n",
    "    :rtype: np.ndarray(shape=(1+d, 1))\n",
    "    \"\"\"\n",
    "    \n",
    "    # START OF YOUR CODE\n",
    "    \n",
    "    raise NotImplementedError(\"Function compute_weights() is not implemented\")   \n",
    "    \n",
    "    # END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>2. Write the prediction function</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(X, w):\n",
    "    \"\"\"\n",
    "    Computes the prediction over a set of observations X using the linear function\n",
    "    characterized by the weight vector w.\n",
    "    X is the input data matrix, already with the leftmost column with 1s.\n",
    "\n",
    "    :param X: input data matrix\n",
    "    :type X: np.ndarray(shape=(N, 1+d))\n",
    "    :param w: weight vector\n",
    "    :type w: np.ndarray(shape=(1+d, 1))\n",
    "    :return: regression prediction\n",
    "    :rtype: np.ndarray(shape=(N, 1))\n",
    "    \"\"\"\n",
    "    \n",
    "    # START OF YOUR CODE\n",
    "    \n",
    "    raise NotImplementedError(\"Function make_prediction() is not implemented\")\n",
    "    \n",
    "    # END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the functions\n",
    "\n",
    "We will split the dataset into training and testing parts. We will compute the weight vector using the training part and see how it fits to the test data. We will compute fitting metrics along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Let us fisrt split the dataset into training and testing parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the training data\n",
    "plt.scatter(X_train[:,1], y_train, alpha=0.4)\n",
    "plt.xlabel(colname)\n",
    "plt.ylabel('median value in $1000')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the testing data\n",
    "plt.scatter(X_test[:,1], y_test, alpha=0.4)\n",
    "plt.xlabel(colname)\n",
    "plt.ylabel('median value in $1000')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>3. Training (Finding $\\mathbf{w}$)</mark>\n",
    "\n",
    "Solve the problem for the training data. Use the weight computation function.\n",
    "Print the resulting weight vector and plot the linear function on top of the data points.\n",
    "You should get a plot like the one below:\n",
    "\n",
    "<img src=\"https://www.ime.usp.br/~nina/foo.png\" alt=\"drawing\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the problem using the training data\n",
    "# Print the weight vector\n",
    "# Plot the linear function on top of the data points\n",
    "\n",
    "# START OF YOUR CODE\n",
    "w = 0  # this is temporary\n",
    "\n",
    "\n",
    "print(\"Estimated w =\\n\", w)\n",
    "\n",
    "# END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting metrics\n",
    "\n",
    "Besides the MSE loss, we can use the [$R^2$](https://pt.wikipedia.org/wiki/R%C2%B2) metric (Coefficient of determination) to evaluate how well the linear model fits the data.\n",
    "\n",
    "Use [r2_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) of the [scikit-learn](https://scikit-learn.org/) library to compute the coefficient of determination (R2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>4. Write the MSE computation function</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y, yhat):\n",
    "    \"\"\"\n",
    "    Computes the mean squared error (MSE) loss.\n",
    "\n",
    "    :param y: vector with target output values\n",
    "    :type y: np.ndarray(shape=(N,1))\n",
    "    :param yhat: vector with predicted output values \n",
    "    :type yhat: np.ndarray(shape=(N, 1))\n",
    "    :return: mse\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    \n",
    "    # START OF YOUR CODE\n",
    "    \n",
    "    raise NotImplementedError(\"Function normal_equation_prediction() is not implemented\")\n",
    "    \n",
    "    # END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>5. Compute MSE and R2 on the training data</mark>\n",
    "\n",
    "Print MSE and R2 computed on the training data. Use the prediction and MSE loss computation functions you wrote above and the r2_score function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# START OF YOUR CODE\n",
    "\n",
    "# Replace the 0s below appropriately\n",
    "print(\"MSE = %f\"%(0)) \n",
    "print(\"R2 = %f\"%(0))\n",
    "\n",
    "# END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>6. Evaluate the model on the test data</mark>\n",
    "\n",
    "Print MSE and R2 computed the on test data. Use the prediction and MSE loss computation functions you wrote above and the r2_score function. Also, plot the datapoints of the test set and the linear function on top of them.\n",
    "\n",
    "Compare these metrics on the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START OF YOUR CODE\n",
    "\n",
    "\n",
    "# Replace the 0s below appropriately\n",
    "print(\"MSE = %f\" %(0))\n",
    "print(\"R2 = %f\"%(0))\n",
    "\n",
    "\n",
    "# END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. <mark>Testing cases with $d>1$</mark>\n",
    "\n",
    "Now, let us consider input data that consists of multiple features.\n",
    "Your code for computing the weights and making the prediction should work for $d>1$ too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT\n",
    "\n",
    "# Choose two or more features and write the code to build matrix X and target vector y\n",
    "# For instance, try 'RM' and 'LSTAT'\n",
    "\n",
    "# START OF YOUR CODE\n",
    "\n",
    "# END OF YOUR CODE\n",
    "\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to perform the linear regression on the new dataset built in the above cell.\n",
    "# This time there is no need to split the data into training and testing.\n",
    "\n",
    "# The weight computation and prediction funcstions written above should work without mofdification\n",
    "# Print the estimated weight vector, the MSE loss and the coefficient of determination R2\n",
    "    \n",
    "# START OF YOUR CODE\n",
    "\n",
    "# Replace the 0s below appropriately\n",
    "print(\"Estimated w =\\n\", 0)\n",
    "print(\"MSE = %f\" %(0))\n",
    "print(\"R2 = %f\"%(0)\n",
    "\n",
    "# END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional extra\n",
    "You may add new cells below to show other results/things you think are interesting to report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
